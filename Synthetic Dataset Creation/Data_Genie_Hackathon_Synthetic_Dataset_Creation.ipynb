{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTING NECESSARY LIBRARIES**"
      ],
      "metadata": {
        "id": "RhQMnjWr_yUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima_process import ArmaProcess\n",
        "from statsmodels.tsa.arima_process import ArmaProcess\n",
        "from random import randint\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "08q_gy6y_2by"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:**\n",
        "Basic Idea of Synthetic data geneation here is to create synthetic time series data considering the 7 models under consideration\n",
        "\n",
        "\n",
        "1.   ARIMA\n",
        "2.   ETS\n",
        "3.   GARCH\n",
        "4.   LSTM\n",
        "5.   PROPHET\n",
        "6.   SARIMAX\n",
        "7.   STL\n",
        "\n",
        "such that these models are the best fit for their respective synthetic dataset created. By this means the datasets generated will be representative of the uniqueness of each of the model. So for each model we generate 5000 synthetic random time series data for which the corresponding model is the best.\n",
        "\n",
        "**Step 2:**\n",
        "\n",
        "Now once the time series data are available I prepared a feature extraction pipeline for each of the time series data supplied and generate all statistical and time series related feaytures from the dataset such that we get a single feature vector for each time series dataset. With these genearated feature dataset each mapped to the corresponding time series models then we try to build the classification model with it\n",
        "\n"
      ],
      "metadata": {
        "id": "YC_E-4srSl7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Space Constructed from the time series data (Sample)**"
      ],
      "metadata": {
        "id": "aT9iOkqUSXI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a total I have generated 96 features from each of the time series data. I have built a feature extraction pipeline for this which when supplied a time series data will convert the data frame to this feature space.\n",
        "\n",
        "This pipeline can be reached by sending a POST request to this endpoint\n",
        "\n",
        "[http://ec2-13-233-69-101.ap-south-1.compute.amazonaws.com:80/data/transform](http://ec2-13-233-69-101.ap-south-1.compute.amazonaws.com:80/data/transform)\n",
        "\n"
      ],
      "metadata": {
        "id": "1Hcxwx2ESksW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    \"Mean\": -3.5202193463724476e-17,\n",
        "    \"Median\": -0.02275393954494072,\n",
        "    \"Mode\": -3.153297362713809,\n",
        "    \"Variance\": 0.9999999999999998,\n",
        "    \"Std Dev\": 0.9999999999999999,\n",
        "    \"Skewness\": -0.06439507844883287,\n",
        "    \"Kurtosis\": 0.49304717187986036,\n",
        "    \"Linear Regression Slope\": 0.014310287489994382,\n",
        "    \"25th Percentile\": -0.6308659146302897,\n",
        "    \"75th Percentile\": 0.6062092651014611,\n",
        "    \"Auto_Corr_Lag1\": 0.34039891321081905,\n",
        "    \"Auto_Corr_Lag2\": -0.05650784384377178,\n",
        "    \"Rolling_Mean\": 0.41183730445661576,\n",
        "    \"Rolling_Std_Dev\": 0.5334082716360593,\n",
        "    \"Seasonality Present\": True,\n",
        "    \"Seasonal Mean\": -0.0034075888177623396,\n",
        "    \"Seasonal Variance\": 0.04858148003339914,\n",
        "    \"Seasonal Max\": 0.39173300613889395,\n",
        "    \"Seasonal Min\": -0.326085833237132,\n",
        "    \"Seasonal 25th percentile\": -0.11231072308238275,\n",
        "    \"Seasonal 75th percentile\": 0.12408666805787795,\n",
        "    \"Trend Mean\": 0.0009074204943658445,\n",
        "    \"Trend Variance\": 0.21865405560996645,\n",
        "    \"Trend Max\": 0.7249434566895762,\n",
        "    \"Trend Min\": -1.1767955653929383,\n",
        "    \"Trend 25th percentile\": 0,\n",
        "    \"Trend 75th percentile\": 0,\n",
        "    \"Residual Mean\": 0.00016195372511192203,\n",
        "    \"Residual Variance\": 0.6702937121624201,\n",
        "    \"Residual Max\": 2.1759509289522363,\n",
        "    \"Residual Min\": -2.1862611097431532,\n",
        "    \"Residual 25th percentile\": 0,\n",
        "    \"Residual 75th percentile\": 0,\n",
        "    \"Seasonal Indices Mean\": -8.0953762212251e-18,\n",
        "    \"Seasonal Indices Variance\": 0.04941311863814058,\n",
        "    \"Seasonal Indices Max\": 0.39173300613889395,\n",
        "    \"Seasonal Indices Min\": -0.326085833237132,\n",
        "    \"Seasonal Indices 25th percentile\": -0.11231072308238275,\n",
        "    \"Seasonal Indices 75th percentile\": 0.12408666805787795,\n",
        "    \"Mean Frequency\": -0.0060975609756097554,\n",
        "    \"Median Frequency\": -0.006097560975609756,\n",
        "    \"Frequency Variance\": 0.08332093991671624,\n",
        "    \"Frequency Skewness\": 5.0740938455380016e-17,\n",
        "    \"Frequency Kurtosis\": -1.2003569834895136,\n",
        "    \"Frequency 25th Percentile\": -0.2530487804878049,\n",
        "    \"Frequency 75th Percentile\": 0.24085365853658536,\n",
        "    \"Fequency Entropy Value\": -1.7976931348623157e+308,\n",
        "    \"Fequency Peak Value\": 0.14634146341463417,\n",
        "    \"Frequency Delta Theta Ratio\": 0,\n",
        "    \"Lag 1 Feature Correlation\": 0.340398913210819,\n",
        "    \"First Order Difference Mean\": 0.0032950356453718514,\n",
        "    \"First Order Difference Median\": 0.02387736123341641,\n",
        "    \"First Order Difference Variance\": 1.3275964219931575,\n",
        "    \"First Order Difference Skewness\": 0.007690568269185233,\n",
        "    \"First Order Difference Kurtosis\": -0.038742141830598875,\n",
        "    \"First Order Difference 25th Percentile\": -0.8103994553537286,\n",
        "    \"First Order Difference 75th Percentile\": 0.8662040270697486,\n",
        "    \"First Order Difference Entropy\": -1.7976931348623157e+308,\n",
        "    \"Information Entropy\": 1.9810014688665833,\n",
        "    \"Sample Entropy\": 1.9810014688665833,\n",
        "    \"cA Coeffecients Mean\": -4.332577657073782e-17,\n",
        "    \"cA Coeffecients Variance\": 1.3032392572774771,\n",
        "    \"cA Coeffecients Max\": 2.5707386350491066,\n",
        "    \"cA Coeffecients Min\": -3.1998969650478672,\n",
        "    \"cA Coeffecients 25th percentile\": -0.5866117260741085,\n",
        "    \"cA Coeffecients 75th percentile\": 0.5437116777270998,\n",
        "    \"cD Coeffecients Mean\": -0.02137463928012771,\n",
        "    \"cD Coeffecients Variance\": 0.6963038675181672,\n",
        "    \"cD Coeffecients Max\": 2.3099388911930543,\n",
        "    \"cD Coeffecients Min\": -2.0402919482802306,\n",
        "    \"cD Coeffecients 25th percentile\": -0.6124987414321151,\n",
        "    \"cD Coeffecients 75th percentile\": 0.5720424602257506,\n",
        "    \"acf_values_0\": 1.0,\n",
        "    \"acf_values_1\": 0.33845271570521984,\n",
        "    \"acf_values_2\": -0.05596237161267924,\n",
        "    \"acf_values_3\": 0.022097869239583814,\n",
        "    \"acf_values_4\": -0.1474901522711735,\n",
        "    \"acf_values_5\": -0.06457924588311154,\n",
        "    \"pacf_values_0\": 1.0,\n",
        "    \"pacf_values_1\": 0.34263114429417296,\n",
        "    \"pacf_values_2\": -0.19800222064429937,\n",
        "    \"pacf_values_3\": 0.13481587579162574,\n",
        "    \"pacf_values_4\": -0.271208476632644,\n",
        "    \"pacf_values_5\": 0.15495770168773404,\n",
        "    \"num_acf_peaks\": 4,\n",
        "    \"dominant_frequency\": 1.0,\n",
        "    \"strength_of_seasonality_acf\": 0.33845271570521984,\n",
        "    \"has_multiple_seasonal_patterns\": True,\n",
        "    \"poly_coefficients_0\": 0.0,\n",
        "    \"poly_coefficients_1\": 0.037975949394819936,\n",
        "    \"poly_coefficients_2\": -0.000292168665491678,\n",
        "    \"residuals_normality_pvalue\": 0.9637170781846063,\n",
        "    \"stationary\": False,\n",
        "    \"significant_acf_peaks\": 2,\n",
        "    \"significant_pacf_peaks\": 3,\n",
        "    \"lag1_correlation\": 0.34039891321081905\n",
        "}"
      ],
      "metadata": {
        "id": "I3RnvP_bST5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code for generation of synthetic datasets correspodning to each of the model"
      ],
      "metadata": {
        "id": "lt9SbMHIVADe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ARIMA SYNTHETIC DATA GENERATION**"
      ],
      "metadata": {
        "id": "5QevwjEAJjM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of time series to generate\n",
        "num_series = 10000\n",
        "n_samples = 750  # Number of data points per series\n",
        "\n",
        "# Start date for the time series\n",
        "start_date = datetime(2021, 1, 1)\n",
        "\n",
        "# Initialize an empty DataFrame to store the synthetic time series data\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Function to generate a single time series\n",
        "def generate_time_series(seed):\n",
        "    # Randomly select p and q values\n",
        "    p = max(1, randint(0, 3))  # Autoregressive order (at least 1)\n",
        "    q = max(1, randint(0, 3))  # Moving average order (at least 1)\n",
        "\n",
        "    # Define ARMA parameters (p, q) and coefficients\n",
        "    ar_coefs = np.random.uniform(-0.9, 0.9, p)  # Random AR coefficients\n",
        "    ma_coefs = np.random.uniform(-0.9, 0.9, q)  # Random MA coefficients\n",
        "    print(ar_coefs)\n",
        "    print(ma_coefs)\n",
        "    # Generate ARMA process\n",
        "    arma_process = ArmaProcess(ar_coefs, ma_coefs)\n",
        "\n",
        "    # Generate synthetic time series data\n",
        "    np.random.seed(seed)  # Ensure reproducibility for each series\n",
        "    synthetic_data = arma_process.generate_sample(nsample=n_samples)\n",
        "\n",
        "    # Create a date range for the time series\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
        "\n",
        "    return date_range, synthetic_data\n",
        "\n",
        "# Create synthetic time series data and add to the DataFrame\n",
        "for i in range(num_series):\n",
        "    df = pd.DataFrame()\n",
        "    date_range, synthetic_data = generate_time_series(i)\n",
        "    df['Date'] = date_range\n",
        "    df['Value'] = synthetic_data\n",
        "    df.to_csv(f\"DataGenie Data/ARIMA DATA/arima_sample_{i}.csv\",index=False,header=True)"
      ],
      "metadata": {
        "id": "4K-rm36ENHVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame()\n",
        "path = f\"DataGenie Data/ARIMA DATA/arima_sample\"\n",
        "for i in range(5000):\n",
        "  if i%100==0:\n",
        "    print(i)\n",
        "  filename = path+f\"_{i}.csv\"\n",
        "  data = pd.read_csv(filename,header=0)\n",
        "  formatted_data = {\n",
        "  \"data\": data.to_dict(orient=\"records\")\n",
        "  }\n",
        "  json_string = json.dumps(formatted_data)\n",
        "  result = requests.post(\"http://localhost:80/data/transform\",data=json_string)\n",
        "  result = result.json()\n",
        "  result['Label'] = \"ARIMA\"\n",
        "  result = pd.Series(result)\n",
        "  df = pd.concat([df, result], axis=1)\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "gPiw6PzPKoez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = df.T"
      ],
      "metadata": {
        "id": "PRe4iiKzP_rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.dropna(inplace=True)\n",
        "df_data.shape"
      ],
      "metadata": {
        "id": "XxGscqfNLGZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.to_csv('DataGenie Data/Final Data/ARIMA.csv',header=True,index=False)"
      ],
      "metadata": {
        "id": "c2rI1xVsLGZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ETS SYNTHETIC DATA GENERATION**"
      ],
      "metadata": {
        "id": "It5njbqyJn6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Number of time series to generate\n",
        "num_series = 10000\n",
        "n_samples = 750  # Number of data points per series\n",
        "\n",
        "# Start date for the time series\n",
        "start_date = datetime(2021, 1, 1)\n",
        "\n",
        "# Function to generate a single time series\n",
        "def generate_time_series(seed):\n",
        "    # Generate a random seasonal period value (e.g., between 1 and 12)\n",
        "    seasonal_periods = random.choice([3,6,12])\n",
        "\n",
        "    # Generate synthetic time series data using Exponential Smoothing (ETS)\n",
        "    np.random.seed(seed)  # Ensure reproducibility for each series\n",
        "    synthetic_data = ExponentialSmoothing(np.random.rand(n_samples), seasonal='add', seasonal_periods=seasonal_periods).fit().fittedvalues\n",
        "\n",
        "    # Create a date range for the time series\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
        "\n",
        "    return date_range, synthetic_data, seasonal_periods\n",
        "\n",
        "# Generate synthetic time series data and save each dataset as a separate CSV file\n",
        "for i in range(num_series):\n",
        "    date_range, synthetic_data, seasonal_periods = generate_time_series(i)\n",
        "    df = pd.DataFrame({'Date': date_range, 'Value': synthetic_data})\n",
        "    filename = f\"DataGenie Data/ETS DATA/ets_sample_{i}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "print(\"Data generation completed.\")"
      ],
      "metadata": {
        "id": "89DB7pY1N5zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame()\n",
        "path = f\"DataGenie Data/ETS DATA/ets_sample\"\n",
        "for i in range(5000):\n",
        "  if i%100==0:\n",
        "    print(i)\n",
        "  filename = path+f\"_{i}.csv\"\n",
        "  # print(filename)\n",
        "  data = pd.read_csv(filename,header=0)\n",
        "  formatted_data = {\n",
        "  \"data\": data.to_dict(orient=\"records\")\n",
        "  }\n",
        "  json_string = json.dumps(formatted_data)\n",
        "  result = requests.post(\"http://localhost:80/data/transform\",data=json_string)\n",
        "  result = result.json()\n",
        "  result['Label'] = \"ETS\"\n",
        "  result = pd.Series(result)\n",
        "  df = pd.concat([df, result], axis=1)\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "0FDL_lsmKv8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = df.T"
      ],
      "metadata": {
        "id": "nEi7OXHxQaID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.dropna(inplace=True)\n",
        "df_data.shape"
      ],
      "metadata": {
        "id": "GkVh9TmpK6HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.to_csv('DataGenie Data/Final Data/ETS.csv',header=True,index=False)"
      ],
      "metadata": {
        "id": "tV_OPBqUK-X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PROPHET SYNTHETIC DATA GENERATION**"
      ],
      "metadata": {
        "id": "kEaT6YEeJybl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pystan\n",
        "!pip install --upgrade plotly\n",
        "!pip install fbprophet"
      ],
      "metadata": {
        "id": "T1BZn8TEc0pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Number of time series to generate\n",
        "num_series = 10000\n",
        "n_samples = 750  # Number of data points per series\n",
        "\n",
        "# Start date for the time series\n",
        "start_date = datetime(2021, 1, 1)\n",
        "\n",
        "# Initialize an empty DataFrame to store the synthetic time series data\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Function to generate a single time series\n",
        "def generate_time_series(seed):\n",
        "    # Generate synthetic time series data using a Prophet-like model\n",
        "    np.random.seed(seed)  # Ensure reproducibility for each series\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
        "    synthetic_data = np.random.randn(n_samples)  # Replace with your data generation logic\n",
        "\n",
        "    # Create a Prophet-compatible DataFrame\n",
        "    df_prophet = pd.DataFrame({'ds': date_range, 'y': synthetic_data})\n",
        "\n",
        "    # Create and fit a Prophet model\n",
        "    model = Prophet()\n",
        "    model.fit(df_prophet)\n",
        "\n",
        "    # Generate future dates for forecasting\n",
        "    future = model.make_future_dataframe(periods=n_samples)\n",
        "\n",
        "    # Make forecasts\n",
        "    forecast = model.predict(future)\n",
        "\n",
        "    # Extract the fitted values as the synthetic time series data\n",
        "    synthetic_data = forecast['yhat'][-n_samples:].values\n",
        "\n",
        "    return date_range, synthetic_data\n",
        "\n",
        "# Generate synthetic time series data and add to the DataFrame\n",
        "for i in range(num_series):\n",
        "    date_range, synthetic_data = generate_time_series(i)\n",
        "    df = pd.DataFrame({'Date': date_range, 'Value': synthetic_data})\n",
        "    filename = f\"DataGenie Data/PROPHET DATA/prophet_sample_{i}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "print(\"Data generation completed.\")"
      ],
      "metadata": {
        "id": "i4mTDnPJT7bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame()\n",
        "path = f\"DataGenie Data/PROPHET DATA/prophet_sample\"\n",
        "for i in range(5000):\n",
        "  if i%100==0:\n",
        "    print(i)\n",
        "  filename = path+f\"_{i}.csv\"\n",
        "  # print(filename)\n",
        "  data = pd.read_csv(filename,header=0)\n",
        "  formatted_data = {\n",
        "  \"data\": data.to_dict(orient=\"records\")\n",
        "  }\n",
        "  json_string = json.dumps(formatted_data)\n",
        "  result = requests.post(\"http://localhost:80/data/transform\",data=json_string)\n",
        "  result = result.json()\n",
        "  result['Label'] = \"PROPHET\"\n",
        "  result = pd.Series(result)\n",
        "  df = pd.concat([df, result], axis=1)\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "y2zZId4sLSLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = df.T"
      ],
      "metadata": {
        "id": "F1CrzhJvQDip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.dropna(inplace=True)\n",
        "df_data.shape"
      ],
      "metadata": {
        "id": "_NjmprBCQDip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.to_csv('DataGenie Data/Final Data/PROPHET.csv',header=True,index=False)"
      ],
      "metadata": {
        "id": "kD3e6Ix8QDiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STL SYNTHETIC DATA GENERATION**"
      ],
      "metadata": {
        "id": "D-G8oetSJ7LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Number of time series to generate\n",
        "num_series = 10000\n",
        "n_samples = 750  # Number of data points per series\n",
        "\n",
        "# Start date for the time series\n",
        "start_date = datetime(2021, 1, 1)\n",
        "\n",
        "# Initialize an empty DataFrame to store the synthetic time series data\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Function to generate a single time series\n",
        "def generate_time_series(seed):\n",
        "    # Generate synthetic time series data using STL-like model\n",
        "    np.random.seed(seed)  # Ensure reproducibility for each series\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
        "\n",
        "    # Generate trend component\n",
        "    trend = np.cumsum(np.random.normal(0, 0.1, n_samples))\n",
        "\n",
        "    # Generate seasonal component\n",
        "    seasonal_period = random.choice([6,12])  # Monthly seasonality (you can adjust this)\n",
        "    seasonal = np.sin(2 * np.pi * np.arange(n_samples) / seasonal_period)\n",
        "\n",
        "    # Generate residual (noise) component\n",
        "    residual = np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "    # Combine components to create the synthetic time series\n",
        "    synthetic_data = trend + seasonal + residual\n",
        "\n",
        "    # Create a date range for the time series\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
        "\n",
        "    return date_range, synthetic_data\n",
        "\n",
        "# Generate synthetic time series data and add to the DataFrame\n",
        "for i in range(num_series):\n",
        "    date_range, synthetic_data = generate_time_series(i)\n",
        "    df = pd.DataFrame({'Date': date_range, 'Value': synthetic_data})\n",
        "    filename = f\"DataGenie Data/STL/stl_sample_{i}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "print(\"Data generation completed.\")"
      ],
      "metadata": {
        "id": "TiCIrre7czIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame()\n",
        "path = f\"DataGenie Data/STL/stl_sample\"\n",
        "for i in range(5000):\n",
        "  if i%100==0:\n",
        "    print(i)\n",
        "  filename = path+f\"_{i}.csv\"\n",
        "  date_range, synthetic_data = generate_time_series(i)\n",
        "  data = pd.DataFrame({'Date': date_range, 'Value': synthetic_data})\n",
        "  data['Date'] = data['Date'].astype('str')\n",
        "  data.to_csv(filename,index=False,header=True)\n",
        "  formatted_data = {\n",
        "  \"data\": data.to_dict(orient=\"records\")\n",
        "  }\n",
        "  json_string = json.dumps(formatted_data)\n",
        "  result = requests.post(\"http://localhost:80/data/transform\",data=json_string)\n",
        "  result = result.json()\n",
        "  result['Label'] = \"STL\"\n",
        "  result = pd.Series(result)\n",
        "  df = pd.concat([df, result], axis=1)\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "cH9u_2o6LbPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = df.T"
      ],
      "metadata": {
        "id": "XWQXtk0kQI6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.dropna(inplace=True)\n",
        "df_data.shape"
      ],
      "metadata": {
        "id": "eVhww7mzQI6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.to_csv('DataGenie Data/Final Data/STL.csv',header=True,index=False)"
      ],
      "metadata": {
        "id": "HMvoxkHhQI6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GARCH SYNTHETIC DATA GENERATION**"
      ],
      "metadata": {
        "id": "3ro2zqnxKBMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT75nA15iW3U",
        "outputId": "c3eee0e0-b75b-4ac6-e4e9-8f6d6de55033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arch\n",
            "  Downloading arch-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.7/981.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from arch) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from arch) (1.11.2)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.10/dist-packages (from arch) (1.5.3)\n",
            "Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.10/dist-packages (from arch) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->arch) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->arch) (2023.3.post1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels>=0.12->arch) (1.16.0)\n",
            "Installing collected packages: arch\n",
            "Successfully installed arch-6.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from arch import arch_model\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Number of time series to generate\n",
        "num_series = 10000\n",
        "n_samples = 750  # Number of data points per series\n",
        "\n",
        "# Start date for the time series\n",
        "start_date = datetime(2021, 1, 1)\n",
        "\n",
        "# Initialize an empty DataFrame to store the synthetic time series data\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Function to generate a single time series with random GARCH parameters\n",
        "def generate_time_series(seed):\n",
        "    np.random.seed(seed)  # Ensure reproducibility for each series\n",
        "\n",
        "    # Generate random GARCH(1,1) parameters\n",
        "    omega = np.random.uniform(0.01, 0.1)\n",
        "    alpha = np.random.uniform(0.05, 0.2)\n",
        "    beta = np.random.uniform(0.7, 0.9)\n",
        "\n",
        "    # Initialize arrays to store synthetic data and conditional volatilities\n",
        "    synthetic_data = np.zeros(n_samples)\n",
        "    conditional_volatilities = np.zeros(n_samples)\n",
        "\n",
        "    for t in range(1, n_samples):\n",
        "        # Generate white noise\n",
        "        white_noise = np.random.normal(0, 1)\n",
        "\n",
        "        # Calculate conditional volatility using GARCH(1,1) formula\n",
        "        conditional_volatilities[t] = np.sqrt(omega + alpha * synthetic_data[t - 1] ** 2 + beta * conditional_volatilities[t - 1] ** 2)\n",
        "\n",
        "        # Generate synthetic data point\n",
        "        synthetic_data[t] = conditional_volatilities[t] * white_noise\n",
        "\n",
        "    # Create date range\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
        "\n",
        "    return date_range, synthetic_data\n",
        "\n",
        "# Generate synthetic time series data and add to the DataFrame\n",
        "for i in range(num_series):\n",
        "    date_range, synthetic_data = generate_time_series(i)\n",
        "    df = pd.DataFrame({'Date': date_range, 'Value': synthetic_data})\n",
        "    filename = f\"DataGenie Data/GARCH DATA/garch_sample_{i}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "print(\"Dataset Created Successfully\")"
      ],
      "metadata": {
        "id": "bTAs2ssChAwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame()\n",
        "path = f\"DataGenie Data/GARCH DATA/garch_sample\"\n",
        "for i in range(5000):\n",
        "  if i%100==0:\n",
        "    print(i)\n",
        "  filename = path+f\"_{i}.csv\"\n",
        "  data = pd.read_csv(filename,header=0)\n",
        "  formatted_data = {\n",
        "  \"data\": data.to_dict(orient=\"records\")\n",
        "  }\n",
        "  json_string = json.dumps(formatted_data)\n",
        "  result = requests.post(\"http://localhost:80/data/transform\",data=json_string)\n",
        "  result = result.json()\n",
        "  result['Label'] = \"GARCH\"\n",
        "  result = pd.Series(result)\n",
        "  df = pd.concat([df, result], axis=1)\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "GNqjont0Le_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = df.T"
      ],
      "metadata": {
        "id": "tXuU3NAgQKTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.dropna(inplace=True)\n",
        "df_data.shape"
      ],
      "metadata": {
        "id": "o-PQqoZGQKTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.to_csv('DataGenie Data/Final Data/GARCH.csv',header=True,index=False)"
      ],
      "metadata": {
        "id": "_O9qfE_2QKTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SARIMAX SYNTHETIC DATA GENERATION**"
      ],
      "metadata": {
        "id": "i4XpE6hEKILZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Number of time series datasets to generate\n",
        "num_series = 10000\n",
        "n_samples = 750  # Number of data points per series\n",
        "\n",
        "# Start date for the time series\n",
        "start_date = datetime(2021, 1, 1)\n",
        "\n",
        "# Initialize an empty DataFrame to store the synthetic time series data\n",
        "df = pd.DataFrame()\n",
        "\n",
        "def generate_time_series(seed):\n",
        "    np.random.seed(seed)  # Ensure reproducibility for each series\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
        "\n",
        "    # Define random SARIMA parameters (p, d, q, P, D, Q, s)\n",
        "    p = np.random.randint(0, 3)  # Autoregressive order (0 to 2)\n",
        "    d = np.random.randint(0, 2)  # Differencing order (0 to 1)\n",
        "    q = np.random.randint(0, 3)  # Moving average order (0 to 2)\n",
        "\n",
        "    P = np.random.randint(0, 2)  # Seasonal autoregressive order (0 to 1)\n",
        "    D = np.random.randint(0, 1)  # Seasonal differencing order (0 to 0, typically 0 or 1)\n",
        "    Q = np.random.randint(0, 2)  # Seasonal moving average order (0 to 1)\n",
        "    s = 12  # Fixed seasonality of 12 for monthly data\n",
        "\n",
        "    # Generate synthetic SARIMA time series data\n",
        "    model = SARIMAX(np.random.randn(n_samples), order=(p, d, q), seasonal_order=(P, D, Q, s))\n",
        "    sarima_data = model.simulate(params=np.random.randn(model.k_params), nsimulations=n_samples)\n",
        "\n",
        "    # Create a date range for the time series\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
        "\n",
        "    return date_range, sarima_data\n",
        "\n",
        "# Generate synthetic time series data and add to the DataFrame\n",
        "for i in range(num_series):\n",
        "    date_range, sarima_data = generate_time_series(i)\n",
        "    col_name = f'Series_{i} (SARIMA)'\n",
        "    df = pd.DataFrame({'Date': date_range, 'Value': synthetic_data})\n",
        "    filename = f\"/content/drive/MyDrive/DataGenie Hackathon/SARIMAX DATA/sarimax_sample_{i}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved {filename}\")"
      ],
      "metadata": {
        "id": "X0Z_Qr77iLfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame()\n",
        "path = f\"DataGenie Data/SARIMAX DATA/sarimax_sample\"\n",
        "for i in range(5000):\n",
        "  if i%100==0:\n",
        "    print(i)\n",
        "  filename = path+f\"_{i}.csv\"\n",
        "  date_range, synthetic_data = generate_time_series(i)\n",
        "  data = pd.DataFrame({'Date': date_range, 'Value': synthetic_data})\n",
        "  data['Date'] = data['Date'].astype('str')\n",
        "  data.to_csv(filename,index=False,header=True)\n",
        "  formatted_data = {\n",
        "  \"data\": data.to_dict(orient=\"records\")\n",
        "  }\n",
        "  json_string = json.dumps(formatted_data)\n",
        "  result = requests.post(\"http://localhost:80/data/transform\",data=json_string)\n",
        "  result = result.json()\n",
        "  result['Label'] = \"SARIMAX\"\n",
        "  result = pd.Series(result)\n",
        "  df = pd.concat([df, result], axis=1)\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "Al3J09olRRjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = df.T"
      ],
      "metadata": {
        "id": "XRvO__yXQK5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.dropna(inplace=True)\n",
        "df_data.shape"
      ],
      "metadata": {
        "id": "5h0FyJ8rQK5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.to_csv('DataGenie Data/Final Data/SARIMAX.csv',header=True,index=False)"
      ],
      "metadata": {
        "id": "OHkxWEJMQK5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM SYNTHETIC DATA GENERATION**"
      ],
      "metadata": {
        "id": "ZHMmV0xzKTN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Number of time series datasets to generate\n",
        "num_series = 10000\n",
        "n_samples = 750  # Number of data points per series\n",
        "\n",
        "# Start date for the time series\n",
        "start_date = datetime(2021, 1, 1)\n",
        "\n",
        "# Initialize an empty DataFrame to store the synthetic time series data\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Function to generate a single time series with random patterns\n",
        "def generate_time_series(seed):\n",
        "    np.random.seed(seed)  # Ensure reproducibility for each series\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
        "\n",
        "    # Generate synthetic time series data with random patterns\n",
        "    time = np.arange(0, n_samples)\n",
        "    trend = np.random.uniform(0, 0.1) * time  # Random linear trend\n",
        "    seasonal_pattern = 10 * np.sin(2 * np.pi * time / 30)  # Random seasonal pattern\n",
        "    noise = np.random.normal(0, 1, n_samples)  # Random noise\n",
        "    synthetic_data = trend + seasonal_pattern + noise\n",
        "\n",
        "    # Create a date range for the time series\n",
        "    date_range = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
        "\n",
        "    return date_range, synthetic_data\n",
        "\n",
        "# Generate synthetic time series data for multiple datasets\n",
        "for i in range(num_series):\n",
        "    date_range, synthetic_data = generate_time_series(i)\n",
        "    col_name = f'Series_{i} (LSTM)'\n",
        "    df = pd.DataFrame({'Date': date_range, 'Value': synthetic_data})\n",
        "    filename = f\"DataGenie Data/LSTM DATA/lstm_sample_{i}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved {filename}\")"
      ],
      "metadata": {
        "id": "M3OlmdcUlV6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame()\n",
        "path = f\"DataGenie Data/LSTM DATA/lstm_sample\"\n",
        "for i in range(5000):\n",
        "  if i%100==0:\n",
        "    print(i)\n",
        "  filename = path+f\"_{i}.csv\"\n",
        "  date_range, synthetic_data = generate_time_series(i)\n",
        "  data = pd.DataFrame({'Date': date_range, 'Value': synthetic_data})\n",
        "  data['Date'] = data['Date'].astype('str')\n",
        "  data.to_csv(filename,index=False,header=True)\n",
        "  formatted_data = {\n",
        "  \"data\": data.to_dict(orient=\"records\")\n",
        "  }\n",
        "  json_string = json.dumps(formatted_data)\n",
        "  result = requests.post(\"http://localhost:80/data/transform\",data=json_string)\n",
        "  result = result.json()\n",
        "  result['Label'] = \"LSTM\"\n",
        "  result = pd.Series(result)\n",
        "  df = pd.concat([df, result], axis=1)\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "NAiTgTQM0AYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = df.T"
      ],
      "metadata": {
        "id": "dJT7WQ4SQM_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.dropna(inplace=True)\n",
        "df_data.shape"
      ],
      "metadata": {
        "id": "52XYYwuwQM_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.to_csv('DataGenie Data/Final Data/LSTM.csv',header=True,index=False)"
      ],
      "metadata": {
        "id": "92kkD89LQM_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}